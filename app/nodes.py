import os
from langchain.embeddings import HuggingFaceEmbeddings
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore
from langchain.prompts import MessagesPlaceholder, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, PromptTemplate
from langchain.chains import ConversationalRetrievalChain, create_retrieval_chain, LLMChain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import SystemMessage
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import SystemMessage

class ChoosePathNode:
    def __init__(self, llm):

        self.llm = llm

        self.prompt = """
        Tu es un classifieur. Ton but est de classifié par oui ou non si tu as besoin d'avoir accès à plus d'information juridique,
        ou si tu dois juste répondre à la question.
        Si tu ne vois pas l'utilité d'avoir accès à une base de donnéees juridique pour répondre à la question, répond par "Chat_node",
        si tu as beosin d'avoir accès à une base juridique, répond par "Preprocess_node".

        Chat history:
            {chat_history}

        Question:
            {question}

        Réponse:
            oui/non ?
        """
    def run(self, state: dict):
        """
        Runs the agent to answer a legal question.

        Args:
            question: The legal question to answer.

        Returns:
            The answer to the question, generated by the agent.
        """
        question = state.get("question", "")["question"].strip()
        chat_history = state.get("chat_history", "")
        # Create the prompt template for the agent
        prompt_template = PromptTemplate(
            template = self.prompt,
            input_variables=["question", "chat_history"]
        )

        chain = LLMChain(llm = self.llm, prompt = prompt_template)
        answer = chain.invoke({"question": question, "chat_history": chat_history})

        return {"path": answer["text"]}

class RAGNode:
    """
    This class implements a retrieval-augmented generation (RAG) system for answering
    legal questions using a Pinecone vector store and a language model (LLM).
    """

    def __init__(self, llm, index_name: str, embeddings: HuggingFaceEmbeddings):
        """
        Initializes the RAGLegalExpert.

        Args:
            llm: The language model to use for generating answers.
            index_name: The name of the Pinecone index to use.
            embeddings: The HuggingFaceEmbeddings object for creating embeddings.
        """
        self.llm = llm
        self.index_name = index_name
        self.embeddings = embeddings

        # Connect to Pinecone
        pinecone = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        self.index = pinecone.Index(index_name)
        self.vectorstore = PineconeVectorStore(self.index, embedding=self.embeddings, text_key="document")

        # Prompt templates for system and user messages
        self.prompt_template = """
        Tu es un assistant juridique spécialisé dans la recherche
        d'informations juridique et les questions réponses. Tu es capable de donner des informations sur les procédures pénales,
        les procédures administratives et toutes autres informations d'ordre légale.
        Fournis une réponse complète et informative à la question.

        La réponse devra s'appuyer sur les documents fournis. Ces Documents sont utiles pour répondre à la question.
        Documents:

        Documents:
            {context}

        Question:
            {question}
        """

        # Retriever with similarity threshold for more relevant results
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"score_threshold": 0.6}
        )

    def run(self, state: dict) -> str:
        """
        Runs the RAG system to answer a legal question.

        Args:
            question: The legal question to answer.

        Returns:
            The answer to the question generated by the RAG system.
        """
        question = state.get("question", "")["question"].strip()
        # Create prompt templates
        prompt = PromptTemplate(input_variables=["context", "question"],
                                  template = self.prompt_template)

        # Initialize RAG chain with document combiner
        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)

        # Create retrieval chain
        rag_chain = create_retrieval_chain(self.retriever, question_answer_chain)

        # Invoke the chain and return only the answer
        response = rag_chain.invoke({"input": "",
                                    "question": question})

        return {"answer" : response["answer"]}

class PreprocessQuestionNode:
    """
    This class implements a retrieval-augmented generation (RAG) system for answering
    legal questions using a Pinecone vector store and a language model (LLM).
    """

    def __init__(self, llm):
        """
        Initializes the RAGLegalExpert.

        Args:
            llm: The language model to use for generating answers.
            index_name: The name of the Pinecone index to use.
            embeddings: The HuggingFaceEmbeddings object for creating embeddings.
        """
        self.llm = llm
        # Prompt templates for system and user messages
        self.prompt_template = """

        Tu es un assistant juridique français.
        Tu dois reformuler la question pour y implémenter des mots clefs juridique afin de faciliter
        la recherche de document juridique.

        Ta réponse devra être exaustive et tu mettra en évidence les questions de la plaignante.

        Question:
            {question}
        """

    def run(self, state: dict) -> str:
        """
        Runs the RAG system to answer a legal question.

        Args:
            question: The legal question to answer.

        Returns:
            The answer to the question generated by the RAG system.
        """
        question = state.get("question", "")["question"].strip()
        # Create prompt templates
        prompt = PromptTemplate(
            template = self.prompt_template,
            input_variables=["question"]
        )

        # Initialize RAG chain with document combiner
        llm_chain = LLMChain(llm = self.llm, prompt=prompt)
        answer = llm_chain.invoke(prompt.format(question=question))
        return {"preproccess_question" : answer['text']}


class ChatNode:
    """
    The Node responsable for the chat.
    """
    def __init__(self, llm):
        """
        Initializes the ChatNode.

        Args:
            llm: The language model to use for reasoning and response generation.
        """
        self.llm = llm

        self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    def run(self, state: dict) -> str:
        """
        Runs the agent to answer a legal question.

        Args:
            question: The legal question to answer.

        Returns:
            The answer to the question, generated by the agent.
        """
        context = state.get("context", "")
        question = state.get("question", "")["question"]
        # Create the prompt template for the agent
        if context is not None:
            context = context.strip()
        else:
            context = ""

        if question is not None:
            question = question.strip()
        else:
            question = ""
        # Create the prompt template for the agent

        instructions = f"""
        Tu es un chatbot juridique français spécialisé dans le domaine du droit.
        T'es collègues peuvent te donner des éléments de réponses spécifiques.
        Réponds à la question en utilisant les réponses de tes collègues spécialisés dans le domaine juridique si elles sont disponibles.
        Tu as aussi accès à l'historique du chat.

        Si les réponses de tes collègues ou l'historique de chat ne sont pas disponibles ou ne sont pas suffisantes, utilise tes connaissances pour répondre de manière aussi précise que possible à la question posée.

        Réponse des collègues:
            {context}
        """

        human_message = """
        Question:
            {question}

        Réponse finale:
            Ta réponse:
        """

        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=instructions
                ),  # The persistent system prompt
                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # Where the memory will be stored.
                HumanMessagePromptTemplate.from_template(
                    human_message
                ),  # Where the human input will injected
            ]
        )


        chain = LLMChain(llm = self.llm, prompt = prompt, memory = self.memory)
        final_answer = chain.invoke({"question": question})

        return {"answer": final_answer['chat_history'][-1].content,
                "chat_history": final_answer['chat_history']}
