{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import the required packages for the training."
      ],
      "metadata": {
        "id": "AltcRmGaiv_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTRNewCjwMqU"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers datasets torchmetrics accelerate -Uq\n",
        "!pip install --quiet evaluate wandb mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ghdx3xplD1u"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the required environment keys.\n",
        "(Example using google colab.)"
      ],
      "metadata": {
        "id": "kRQj6InxiGXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu3D_kPsiHgp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "\n",
        "## To use the Eunomia App for evaluation later\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = 'true'\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Eunomia\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model to train."
      ],
      "metadata": {
        "id": "hLR03TMri7oX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QeeZAnK5EEV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAncBKKYxMlc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import transformers\n",
        "\n",
        "\n",
        "class PreprocessDatasetLegalkitShareGPT:\n",
        "    \"\"\"\n",
        "    A class to preprocess the 'MaziyarPanahi/legalkit_sharegpt' dataset for training a model.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        A tokenizer from the Hugging Face Transformers library.\n",
        "    dataset_name : str\n",
        "        The name of the dataset to be loaded and processed.\n",
        "    train_dataset : datasets.Dataset\n",
        "        The tokenized and split training dataset.\n",
        "    eval_dataset : datasets.Dataset\n",
        "        The tokenized and split validation dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        \"\"\"\n",
        "        Initialize the PreprocessDatasetLegalkitShareGPT class with a tokenizer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokenizer : transformers.PreTrainedTokenizer\n",
        "            The tokenizer used for processing text data.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset_name = \"MaziyarPanahi/legalkit_sharegpt\"\n",
        "\n",
        "        # Preprocess the dataset and tokenize it\n",
        "        dataset = self.preprocess_dataset_legalkit_sharegpt()\n",
        "        tokenized_dataset = dataset.map(self.tokenize_function, batched=True)\n",
        "        tokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label\"])\n",
        "\n",
        "        # Split the tokenized dataset into training and validation sets\n",
        "        train_val_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "        self.train_dataset = train_val_dataset[\"train\"]\n",
        "        self.eval_dataset = train_val_dataset[\"test\"]\n",
        "\n",
        "    def preprocess_dataset_legalkit_sharegpt(self) -> Dataset:\n",
        "        \"\"\"\n",
        "        Load and preprocess the 'MaziyarPanahi/legalkit_sharegpt' dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Dataset\n",
        "            A Hugging Face Dataset object containing the processed text and label columns.\n",
        "        \"\"\"\n",
        "        # Load the dataset\n",
        "        data = load_dataset(self.dataset_name)\n",
        "\n",
        "        # Convert the dataset to a pandas DataFrame for easier manipulation\n",
        "        df = pd.DataFrame.from_dict(data['train'])\n",
        "\n",
        "        # Extract the text and label columns from the conversation data\n",
        "        df[\"text\"] = df[\"conversations_with_input\"].apply(lambda x: x[0][\"value\"])\n",
        "        df[\"label\"] = df[\"conversations_with_input\"].apply(lambda x: x[1][\"value\"])\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        df = df.drop(columns=[\"conversations_with_input\", \"conversations\"])\n",
        "\n",
        "        # Convert the pandas DataFrame back to a Hugging Face Dataset\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def tokenize_function(self, examples: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Tokenize the input text and labels.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        examples : dict\n",
        "            A dictionary containing the examples to be tokenized.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary with tokenized input_ids, labels, and attention_mask.\n",
        "        \"\"\"\n",
        "        questions = examples[\"text\"]\n",
        "        responses = examples[\"label\"]\n",
        "\n",
        "        # Tokenize the questions\n",
        "        question_tokens = self.tokenizer(\n",
        "            questions, padding=\"max_length\", truncation=True, max_length=512\n",
        "        )\n",
        "\n",
        "        # Tokenize the responses\n",
        "        response_tokens = self.tokenizer(\n",
        "            responses, padding=\"max_length\", truncation=True, max_length=512\n",
        "        )\n",
        "\n",
        "        # Combine the tokenized inputs and labels\n",
        "        input_ids = question_tokens[\"input_ids\"]\n",
        "        labels = response_tokens[\"input_ids\"]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels,\n",
        "            \"attention_mask\": question_tokens[\"attention_mask\"],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8FL66itAlUc"
      },
      "outputs": [],
      "source": [
        "class ModelParser:\n",
        "    def __init__(self, model):\n",
        "        \"\"\"\n",
        "        Initialize the class with a given model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : object\n",
        "            The model (e.g., a PyTorch or Hugging Face model).\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    def count_parameters(self):\n",
        "        \"\"\"\n",
        "        Count and display the total number of parameters in the model,\n",
        "        as well as the number of trainable parameters.\n",
        "        Also displays the percentage of trainable parameters relative to\n",
        "        the total number of parameters.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "        # Format numbers with thousand separators\n",
        "        total_params_str = f\"{total_params:,}\".replace(\",\", \" \")\n",
        "        trainable_params_str = f\"{trainable_params:,}\".replace(\",\", \" \")\n",
        "\n",
        "        print(f\"Total parameters: {total_params_str}\")\n",
        "        print(f\"Trainable parameters: {trainable_params_str} ({trainable_percentage:.2f}%)\\n\")\n",
        "\n",
        "    def freeze_layers_by_param_count(self, max_trainable_params):\n",
        "        \"\"\"\n",
        "        Freeze the layers of the model until the maximum number of trainable\n",
        "        parameters is reached.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_trainable_params : int\n",
        "            The maximum number of trainable parameters desired.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        current_trainable_params = 0\n",
        "\n",
        "        # Iterate through the layers of the model\n",
        "        for param in self.model.parameters():\n",
        "            if current_trainable_params + param.numel() > max_trainable_params:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                current_trainable_params += param.numel()\n",
        "\n",
        "        print(f\"Final trainable parameters: {self.count_parameters}\")\n",
        "\n",
        "    def freeze_layers_by_name(self, layer_names):\n",
        "        \"\"\"\n",
        "        Freeze the layers of the model based on the provided layer names.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer_names : list of str\n",
        "            A list of layer names to freeze.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if any(layer_name in name for layer_name in layer_names):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        print(f\"Layers frozen: {layer_names}\")\n",
        "\n",
        "    def freeze_all_except_layer(self, layer_name_to_keep):\n",
        "        \"\"\"\n",
        "        Freeze all layers of the model except for the one specified by its name.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer_name_to_keep : str\n",
        "            The name of the layer to keep unfrozen.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if layer_name_to_keep not in name:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "\n",
        "        print(f\"All layers frozen except: {layer_name_to_keep}\")\n",
        "\n",
        "    def train_specific_layers(self, layer_names):\n",
        "        \"\"\"\n",
        "        Freeze the layers of the model based on the provided layer names, while\n",
        "        keeping the specified layers trainable.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer_names : list of str\n",
        "            A list of layer names to keep trainable.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if any(layer_name in name for layer_name in layer_names):\n",
        "                param.requires_grad = True\n",
        "                print(f\"Layer frozen: {name}\")\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def train_lm_head(self):\n",
        "        \"\"\"\n",
        "        Freeze all parameters except for the lm_head layer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if hasattr(self.model, 'lm_head'):\n",
        "            for param in self.model.lm_head.parameters():\n",
        "                param.requires_grad = True\n",
        "            print(\"lm_head has been frozen.\")\n",
        "        else:\n",
        "            print(\"No lm_head found in the model.\")\n",
        "\n",
        "        self.count_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmxzKsX2orfj"
      },
      "source": [
        "To train llama3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7067AOVok-L"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import transformers\n",
        "\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict[str, str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"\n",
        "    Resize the tokenizer and the corresponding model embeddings to account for new special tokens.\n",
        "\n",
        "    This function adds special tokens to the tokenizer, resizes the model's token embeddings,\n",
        "    and initializes the embeddings for the new tokens by averaging the existing token embeddings.\n",
        "\n",
        "    Note: This method may result in the embedding size not being divisible by 64.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    special_tokens_dict : Dict[str, str]\n",
        "        A dictionary of special tokens to be added to the tokenizer.\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        The tokenizer to which the special tokens will be added.\n",
        "    model : transformers.PreTrainedModel\n",
        "        The model whose token embeddings will be resized.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : transformers.PreTrainedModel\n",
        "        The model with resized token embeddings.\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        The tokenizer with added special tokens.\n",
        "    \"\"\"\n",
        "    # Add the special tokens to the tokenizer and resize the model's embeddings\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        # Get the input and output embeddings from the model\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        # Calculate the average of the existing embeddings\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Assign the averaged embeddings to the new tokens\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Default special tokens\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"<s>\"\n",
        "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "# Create a dictionary for special tokens if they are not already set in the tokenizer\n",
        "special_tokens_dict = {}\n",
        "if tokenizer.pad_token is None:\n",
        "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
        "if tokenizer.eos_token is None:\n",
        "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
        "if tokenizer.bos_token is None:\n",
        "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
        "if tokenizer.unk_token is None:\n",
        "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
        "\n",
        "# Resize the tokenizer and model embeddings to accommodate the new special tokens\n",
        "model, tokenizer = smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict=special_tokens_dict,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Serqeh2x6MaK"
      },
      "outputs": [],
      "source": [
        "prep_data = PreprocessDatasetLegalkitShareGPT(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXphZRKVyw_q"
      },
      "outputs": [],
      "source": [
        "train_data = prep_data.train_dataset\n",
        "val_data = prep_data.eval_dataset\n",
        "data_collator = transformers.data.data_collator.default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You might want to select specific layers to finetune."
      ],
      "metadata": {
        "id": "6gt7uZQg17Jz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uswm8k00A-Ef"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count the number of parameters and freeze all layers except the lm_head_layer"
      ],
      "metadata": {
        "id": "FolO5tm92GXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DdxzgvJBAe0"
      },
      "outputs": [],
      "source": [
        "# Créer une instance de ModelInspector\n",
        "inspector = ModelParser(model)\n",
        "\n",
        "# Compter les paramètres\n",
        "inspector.count_parameters()\n",
        "inspector.train_lm_head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyYs23Wg0te2"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.integrations import WandbCallback\n",
        "import torch\n",
        "import numpy as np  # Added import for numpy, used in compute_metrics\n",
        "\n",
        "class ModifiedTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    A custom Trainer class that overrides the compute_loss and compute_metrics methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs: bool = False):\n",
        "        \"\"\"\n",
        "        Compute the loss using the provided model and inputs.\n",
        "\n",
        "        This method overrides the default loss computation by manually\n",
        "        setting the attention mask to be all ones and using the input_ids\n",
        "        as labels.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : transformers.PreTrainedModel\n",
        "            The model used for forward pass and loss computation.\n",
        "        inputs : Dict[str, torch.Tensor]\n",
        "            The input data containing 'input_ids'.\n",
        "        return_outputs : bool, optional\n",
        "            If True, also return the model outputs, by default False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The computed loss value.\n",
        "        \"\"\"\n",
        "        return model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=torch.ones_like(inputs[\"input_ids\"]).bool(),\n",
        "            labels=inputs[\"input_ids\"],\n",
        "        ).loss\n",
        "\n",
        "# Define the training arguments for the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./Model_llama_3_1_8B\",  # Directory for storing model checkpoints\n",
        "    fp16=False,  # Disable 16-bit floating point precision\n",
        "    gradient_accumulation_steps=1,  # Number of gradient accumulation steps\n",
        "    per_device_train_batch_size=2,  # Training batch size per device\n",
        "    learning_rate=1e-4,  # Learning rate for optimizer\n",
        "    evaluation_strategy='no',  # Disable evaluation during training\n",
        "    save_strategy='no',  # Disable saving checkpoints during training\n",
        "    max_steps=300,  # Maximum number of training steps\n",
        "    logging_steps=5,  # Log every 5 steps\n",
        "    report_to=\"mlflow\",  # Report training metrics to Weights & Biases\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the specified model, dataset, and training arguments\n",
        "trainer = ModifiedTrainer(\n",
        "    model=model,  # Model to train\n",
        "    train_dataset=train_data,  # Training dataset\n",
        "    args=training_args,  # Training arguments\n",
        "    data_collator=data_collator,  # Data collator function\n",
        "    # callbacks=[WandbCallback()],  # Callback for integration with Weights & Biases\n",
        "    tokenizer=tokenizer,  # Tokenizer used for encoding the inputs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example with mlflow:"
      ],
      "metadata": {
        "id": "VYgvnWy9dv2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from mlflow import MlflowClient\n",
        "\n",
        "# Set the tracking URI for the MLflow server, which could be a local server or an ngrok URI\n",
        "track_uri: str = \"http://34.242.16.206:8080/\"  # Replace with your specific tracking URI if needed\n",
        "\n",
        "# Initialize the MLflow client with the specified tracking URI\n",
        "mlflow.set_tracking_uri(track_uri)\n",
        "\n",
        "# Create a new experiment in MLflow with the name \"Eunomia\"\n",
        "# client.create_experiment(name=\"Eunomia\")\n",
        "mlflow.set_experiment(\"Eunomia\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lE-nN9iwdur8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run() as run:\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "ddpIdGT-BrwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register your model\n",
        "from transformers import pipeline\n",
        "import mlflow\n",
        "\n",
        "# Create a text-generation pipeline using the fine-tuned model and tokenizer\n",
        "tuned_pipeline = pipeline(\n",
        "    task=\"text-generation\",  # Specify the task for the pipeline\n",
        "    model=trainer.model,     # Use the model from the Trainer instance\n",
        "    tokenizer=tokenizer,     # Use the associated tokenizer\n",
        ")\n",
        "\n",
        "# Start an MLflow run with the specified run ID to log the model\n",
        "with mlflow.start_run(run_id=run.info.run_id):\n",
        "    mlflow.set_experiment(\"Eunomia\")\n",
        "    # Log the fine-tuned model to MLflow and register it under the specified name\n",
        "    model_info = mlflow.transformers.log_model(\n",
        "        transformers_model=tuned_pipeline,       # The text-generation pipeline to log\n",
        "        artifact_path=\"fine_tuned\",              # Directory path within the artifact store\n",
        "        registered_model_name=\"Eunomia-llama-model\",  # Name of the registered model in MLflow\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7y5WxTI_fV8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.decode(val_data[\"labels\"][1], skip_special_tokens=True)\n",
        "preds = tuned_pipeline(inputs, max_length=450)\n",
        "print(f\"question: {inputs}\\n\\n\")\n",
        "print(f\"predictions: {preds}\")"
      ],
      "metadata": {
        "id": "i6gm3V1SWCVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in val_data[:5]:\n",
        "    inputs = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # Générer la prédiction\n",
        "    generated = tuned_pipeline(inputs, max_length=50)[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "aPgw7VrTVU5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on our langgraph app\n",
        "### Clone the git repo with the app"
      ],
      "metadata": {
        "id": "R0U6mOe1gw9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4VpWFPTmS9P",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/H-Gelender/Eunomia.git\n",
        "%cd Eunomia/app\n",
        "!pip install --quiet -r requirements.txt\n",
        "!pip install --quiet rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5zNSyaNoLlE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Initialize HuggingFaceEmbeddings with a specified model for multilingual text embeddings\n",
        "embeddings: HuggingFaceEmbeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJo9Egyncrm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "# Initialize a Client instance for interacting with the Langsmith API\n",
        "client = Client()\n",
        "\n",
        "# Define the name of the dataset to be used\n",
        "dataset_name: str = \"eunomia-Q&A\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhNYizbVpWHV"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines import Pipeline  # Import Pipeline for type hinting\n",
        "\n",
        "# Create a text-to-text generation pipeline using the specified model and tokenizer\n",
        "pipe: Pipeline = pipeline(\n",
        "    task=\"text2text-generation\",  # Define the task for the pipeline\n",
        "    model=trainer.model,          # Use the model from the Trainer instance\n",
        "    tokenizer=tokenizer,          # Use the associated tokenizer\n",
        "    max_length=512                # Set the maximum sequence length for the generated text\n",
        ")\n",
        "\n",
        "# Wrap the Hugging Face pipeline in a LangChain HuggingFacePipeline object\n",
        "llm: HuggingFacePipeline = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You may want to ajust the llm you want to use in your graph"
      ],
      "metadata": {
        "id": "rBUnqcNUhUPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8AUNSour3TR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Dict, TypedDict, Optional\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from nodes import RAGNode, PreprocessQuestionNode, ChatNode, ChoosePathNode\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph import MessageGraph\n",
        "from langgraph.prebuilt.tool_node import ToolNode\n",
        "\n",
        "# Initialize the Google Generative AI model with specific parameters\n",
        "Gllm: GoogleGenerativeAI = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.1)\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"Typed dictionary to define the structure of the graph state.\"\"\"\n",
        "    question: str\n",
        "    preproccess_question: str\n",
        "    books: str\n",
        "    documents: str\n",
        "    ids: str\n",
        "    answer: str\n",
        "    chat_history: str\n",
        "    path: str\n",
        "\n",
        "class EunomiaGraph:\n",
        "    \"\"\"Class to define and manage the Eunomia state graph with specific nodes.\"\"\"\n",
        "\n",
        "    def __init__(self, llm: GoogleGenerativeAI, embeddings: object):\n",
        "        \"\"\"\n",
        "        Initialize the EunomiaGraph with a language model and embeddings.\n",
        "\n",
        "        Args:\n",
        "            llm: The language model instance to use.\n",
        "            embeddings: The embeddings to use in the RAGNode.\n",
        "        \"\"\"\n",
        "        self.embeddings = embeddings\n",
        "        self.llm = llm\n",
        "        self.workflow = StateGraph(GraphState)\n",
        "        self.app = self.init_graph()\n",
        "\n",
        "        # Initialize lists for storing document IDs, books, and documents\n",
        "        self.ids: list = []\n",
        "        self.books: list = []\n",
        "        self.documents: list = []\n",
        "\n",
        "    def init_node(self):\n",
        "        \"\"\"\n",
        "        Initialize nodes and add them to the workflow.\n",
        "        \"\"\"\n",
        "        self.rag_node = RAGNode(self.llm, \"eunomia\", self.embeddings)\n",
        "        preprocess_node = PreprocessQuestionNode(self.llm)\n",
        "        chat = ChatNode(self.llm)\n",
        "        choose_path = ChoosePathNode(Gllm)\n",
        "\n",
        "        # Add nodes to the workflow\n",
        "        self.workflow.add_node(\"ChoosePath_node\", choose_path.run)\n",
        "        self.workflow.add_node(\"Preprocess_node\", preprocess_node.run)\n",
        "        self.workflow.add_node(\"RAG\", self.rag_node.run)\n",
        "        self.workflow.add_node('Retriever_node', self.retriever_node)\n",
        "        # self.workflow.add_node('Final_Node', self.final_node)\n",
        "        self.workflow.add_node(\"Chat_node\", chat.run)\n",
        "\n",
        "    def init_edges(self):\n",
        "        \"\"\"\n",
        "        Define edges and set entry points in the workflow.\n",
        "        \"\"\"\n",
        "        self.workflow.set_entry_point(\"ChoosePath_node\")\n",
        "        self.workflow.add_conditional_edges(\n",
        "            \"ChoosePath_node\",\n",
        "            self.path,\n",
        "            {\n",
        "                \"Preprocess_node\": \"Preprocess_node\",\n",
        "                \"Chat_node\": \"Chat_node\"\n",
        "            }\n",
        "        )\n",
        "        self.workflow.add_edge(\"Preprocess_node\", \"RAG\")\n",
        "        self.workflow.add_edge(\"RAG\", \"Retriever_node\")\n",
        "        self.workflow.add_edge(\"Retriever_node\", \"Chat_node\")\n",
        "        self.workflow.add_edge(\"Chat_node\", END)\n",
        "        # self.workflow.add_edge(\"Final_Node\", END)\n",
        "\n",
        "    def init_graph(self) -> StateGraph:\n",
        "        \"\"\"\n",
        "        Initialize nodes and edges, then compile and return the workflow graph.\n",
        "\n",
        "        Returns:\n",
        "            StateGraph: The compiled workflow graph.\n",
        "        \"\"\"\n",
        "        self.init_node()\n",
        "        self.init_edges()\n",
        "        return self.workflow.compile()\n",
        "\n",
        "    def retriever_node(self, state: Dict[str, Optional[Dict[str, str]]]) -> Dict[str, list]:\n",
        "        \"\"\"\n",
        "        Process the state to retrieve documents and their metadata.\n",
        "\n",
        "        Args:\n",
        "            state: The current state of the graph containing a question.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains lists of IDs, books, and documents.\n",
        "        \"\"\"\n",
        "        ids: list = []\n",
        "        books: list = []\n",
        "        documents: list = []\n",
        "\n",
        "        # Retrieve the question from the state and invoke the retriever\n",
        "        question = state.get(\"question\", {}).get(\"question\", \"\").strip()\n",
        "        docs = self.rag_node.retriever.invoke(question)\n",
        "\n",
        "        # Collect document metadata and content\n",
        "        for doc in docs:\n",
        "            ids.append(doc.metadata[\"id\"])\n",
        "            books.append(doc.metadata[\"book\"])\n",
        "            documents.append(doc.page_content)\n",
        "\n",
        "        # Update instance variables\n",
        "        self.ids = ids\n",
        "        self.books = books\n",
        "        self.documents = documents\n",
        "\n",
        "        return {\"ids\": ids, \"books\": books, \"documents\": documents}\n",
        "\n",
        "    def path(self, state: Dict[str, str]) -> str:\n",
        "        \"\"\"\n",
        "        Retrieve the path from the state.\n",
        "\n",
        "        Args:\n",
        "            state: The current state of the graph.\n",
        "\n",
        "        Returns:\n",
        "            str: The path from the state.\n",
        "        \"\"\"\n",
        "        return state.get(\"path\", \"\")\n",
        "\n",
        "    def run(self, question: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Run the workflow with a given question and return the result.\n",
        "\n",
        "        Args:\n",
        "            question: The question to be processed.\n",
        "\n",
        "        Returns:\n",
        "            dict: The result of the workflow containing the answer.\n",
        "        \"\"\"\n",
        "        inputs: Dict[str, str] = {\"question\": question}\n",
        "        result: Dict[str, str] = self.app.invoke(inputs)\n",
        "        return result.get(\"answer\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqSKsc-Io2LW"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "from langchain.evaluation import EvaluatorType\n",
        "\n",
        "# Configure the evaluation settings\n",
        "eval_config: RunEvalConfig = RunEvalConfig(\n",
        "    eval_llm=llm,\n",
        "    evaluators=[\n",
        "        RunEvalConfig.Criteria(\"conciseness\"),\n",
        "        RunEvalConfig.Criteria(\"relevance\"),\n",
        "        RunEvalConfig.Criteria(\"coherence\"),\n",
        "        EvaluatorType.STRING_DISTANCE,\n",
        "        EvaluatorType.QA,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define dataset parameters\n",
        "dataset_name: str = \"eunomia-Q&A\"\n",
        "dataset_id: str = \"9f51d387-e223-4998-86b9-145d5d8252ca\"\n",
        "\n",
        "# Run evaluation on the dataset\n",
        "result = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=EunomiaGraph(llm, embeddings).run,\n",
        "    dataset_id=dataset_id,\n",
        "    evaluation=eval_config,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}